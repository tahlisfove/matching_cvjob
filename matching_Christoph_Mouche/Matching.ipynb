{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49137726-2ac9-4808-8447-3d06619e019a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Description ID: 0 | Resume ID: 0 | Similarity: 0.86533636\n",
      "Job Description ID: 0 | Resume ID: 1 | Similarity: 0.9673733\n",
      "Job Description ID: 0 | Resume ID: 2 | Similarity: 0.96490103\n",
      "Job Description ID: 0 | Resume ID: 3 | Similarity: 0.84148645\n",
      "Job Description ID: 0 | Resume ID: 4 | Similarity: 0.93803895\n",
      "Job Description ID: 0 | Resume ID: 5 | Similarity: 0.93672115\n",
      "Job Description ID: 0 | Resume ID: 6 | Similarity: 0.8829583\n",
      "Job Description ID: 0 | Resume ID: 7 | Similarity: 0.84338295\n",
      "Job Description ID: 0 | Resume ID: 8 | Similarity: 0.9190512\n",
      "Job Description ID: 0 | Resume ID: 9 | Similarity: 0.74939895\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "\n",
    "# Charger le tokenizer BERT\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Charger le modèle BERT pré-entraîné\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Charger les données\n",
    "job_descriptions = pd.read_csv(\"jobs_descriptions.csv\")\n",
    "resume = pd.read_csv(\"resume.csv\")\n",
    "\n",
    "# Nettoyer le texte\n",
    "def preprocess_text(text):\n",
    "    # Convertir en chaînes de caractères\n",
    "    text = text.astype(str)\n",
    "    # Convertir en minuscules\n",
    "    text = text.str.lower()\n",
    "    # Supprimer les caractères spéciaux\n",
    "    text = text.str.replace(r'[@#&$%]', '', regex=True)\n",
    "    text = text.str.replace(r'[,.;]', '', regex=True)\n",
    "    text = text.apply(lambda x: [word for word in x if word.lower()])\n",
    "    # Lemmatisation\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = text.apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "    # Rejoindre les mots en une seule chaîne\n",
    "    text = text.apply(lambda x: ' '.join(x))\n",
    "    return text\n",
    "\n",
    "# Prétraiter les descriptions d'emploi et les CV\n",
    "job_descriptions['clean_text'] = preprocess_text(job_descriptions.iloc[:, 2])\n",
    "resume['clean_text'] = preprocess_text(resume['Resume_str'])\n",
    "\n",
    "# Fonction pour obtenir les embeddings des phrases avec BERT\n",
    "def get_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    return embeddings\n",
    "\n",
    "# Limiter le nombre de CV à 20\n",
    "num_resumes = 20\n",
    "# Batch size for processing resumes\n",
    "batch_size = 10\n",
    "# Maximum sequence length\n",
    "max_seq_length = 64\n",
    "# Définir un seuil de similarité\n",
    "threshold = 0.7\n",
    "# Calculate job embeddings once outside the loop\n",
    "job_embeddings = get_embeddings(job_descriptions['clean_text'].tolist())\n",
    "\n",
    "# Attribuer les correspondances en fonction de la similarité\n",
    "matches = []\n",
    "for i, job_desc in enumerate(job_descriptions['clean_text']):\n",
    "    for j in range(0, num_resumes, batch_size):\n",
    "        # Process resumes in batches\n",
    "        batch_resume_texts = resume['clean_text'].iloc[j:j+batch_size].tolist()\n",
    "        batch_resume_embeddings = get_embeddings(batch_resume_texts)\n",
    "\n",
    "        # Calculate similarity for the current batch\n",
    "        similarity_scores = cosine_similarity(job_embeddings[i:i+1], batch_resume_embeddings)\n",
    "        \n",
    "        # Identify matches in the current batch\n",
    "        for k in range(len(batch_resume_texts)):\n",
    "            similarity_score = similarity_scores[0, k]\n",
    "            if similarity_score > threshold:\n",
    "                matches.append((i, j+k, similarity_score))\n",
    "\n",
    "        # Clear variables from memory\n",
    "        del batch_resume_texts, batch_resume_embeddings, similarity_scores\n",
    "\n",
    "# Afficher les correspondances (limit to a certain number of matches)\n",
    "num_matches_to_display = min(10, len(matches))\n",
    "for match in matches[:num_matches_to_display]:\n",
    "    print(\"Job Description ID:\", match[0], \"| Resume ID:\", match[1], \"| Similarity:\", match[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cbc98e-5013-4d1c-b1ae-49119d2a6538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
